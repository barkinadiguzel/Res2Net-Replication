# ğŸŒ¿ Res2Net PyTorch Implementation

This repository contains a PyTorch implementation of **Res2Net**, integrating the **Res2Net module** for **hierarchical multi-scale feature extraction** within residual networks. The model is designed to capture **both local and global features** efficiently while maintaining comparable computational cost.  

- Implemented **Res2Net** with **hierarchical residual-like connections** and optional **SE blocks**.  
- Architecture:  
**Stem â†’ Res2NetResidualBlocks (+SE optional) â†’ GlobalAvgPool â†’ Flatten â†’ FC**

> **Note on Res2Net module:** Each module splits input features into `s` subsets, applies **3Ã—3 convolutions hierarchically**, and merges them via concatenation. This design allows the network to learn **multi-scale receptive fields** dynamically.

**Paper reference:** [Res2Net: A New Multi-scale Backbone Architecture](https://arxiv.org/abs/1904.01169) ğŸ¢

---

## ğŸ–¼ Overview â€“ Res2Net Architecture

![Figure 1](images/figures.jpg)  
*FigureÂ 1:* Multi-scale representations are key for vision tasks, recognizing boundaries,
regions, object categories, and context ('on the table' helps identify objects).

*FigureÂ 2:* (a) Bottleneck block: standard 1x1 + 3x3 conv residual block.
(b) Res2Net (s=4): splits feature maps, each subset (i>1) summed with previous output,
3x3 conv, then concatenated via 1x1 conv.

*FigureÂ 3:* Res2Net integration:
- Cardinality: use group conv (groups = c)
- SE block: recalibrates channel-wise features before residual connection

*FigureÂ 4:* Grad-CAM ResNet-50 vs Res2Net-50:
- Lighter = stronger activation
- Res2Net-50 better activates small objects (baseball, penguin)
- Covers whole large objects (bulbul, mountain dog, ballpoint, mosque)

---

## ğŸ§® Key Mathematical Idea

![Math Concept](images/math.jpg)  
- Split input feature map $$X$$ into $$s$$ subsets: 
$$X = [x_1, x_2, \dots, x_s]$$  

- Process each subset recursively:  
  - $$y_1 = x_1$$  
  - $$y_2 = K_2(x_2)$$  
  - $$y_i = K_i(x_i + y_{i-1}), \quad i = 3, \dots, s$$  

- Concatenate outputs and optionally apply SE: 
$$Y = \text{concat}(y_1, \dots, y_s) \quad \rightarrow \quad SE(Y)$$


This hierarchical splitting enables **combinatorial expansion of receptive fields** within a single block.

---

## ğŸ—ï¸ Model Architecture

```bash
Res2Net/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv_layer.py             # Standard 1x1 and 3x3 convolutions
â”‚   â”‚   â”œâ”€â”€ res2net_block.py          # Res2Net module (split + hierarchical residual connections)
â”‚   â”‚   â”œâ”€â”€ se_block.py               # Optional Squeeze-and-Excitation
â”‚   â”‚   â”œâ”€â”€ flatten_layer.py          # Flatten for classifier
â”‚   â”‚   â”œâ”€â”€ fc_layer.py               # Fully connected classifier
â”‚   â”‚   â”œâ”€â”€ pool_layers/
â”‚   â”‚   â”‚   â”œâ”€â”€ maxpool_layer.py      # MaxPool
â”‚   â”‚   â”‚   â””â”€â”€ avgpool_layer.py      # Global/AdaptiveAvgPool for SE or feature fusion
â”‚   â”‚
â”‚   â”œâ”€â”€ blocks/
â”‚   â”‚   â”œâ”€â”€ res2net_residual.py       # Residual block integrating Res2Net module
â”‚   â”‚   â””â”€â”€ bottleneck_block.py       # Optional baseline bottleneck
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ res2net_model.py          # Full Res2Net: Stem + Res2NetResidualBlocks + SE + Classifier
â”‚   â”‚
â”‚   â””â”€â”€ config.py                      # Input size, num_classes, depth, scale s, cardinality c, SE reduction ratio
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ table1.jpg                     # Top-1/Top-5 accuracy and parameters
â”‚   â”œâ”€â”€ figures.jpg                    # Figures 1-4 from the paper
â”‚   â””â”€â”€ math.jpg                       # Key Res2Net equations and hierarchical splits
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---


## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
